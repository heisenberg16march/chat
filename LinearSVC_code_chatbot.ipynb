{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f59604ed-9bcf-4c2c-84c4-9d5ad2eefe7f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\dell\\anaconda3\\lib\\site-packages (2.0.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\dell\\anaconda3\\lib\\site-packages (1.3.0)\n",
      "Requirement already satisfied: nltk in c:\\users\\dell\\anaconda3\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pandas) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from scikit-learn) (1.11.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: click in c:\\users\\dell\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in c:\\users\\dell\\anaconda3\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\dell\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas scikit-learn nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e197526-ebbf-468d-9085-fe2ee71553de",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in dataset:\n",
      " Context     0\n",
      "Response    4\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: close friend relative whos opinion trust ask honest feedback see like cousin don’t see qualities may cousin’s issue she’s taking always ask would best way communicate plans know specifically expecting meet expectations still blames comments inconsiderate issue coming place loveeven resemble comments way sounds like approaches problem love fact youre introspective enough consider behavior suggests problem\n",
      "Chatbot: new york new yorkits boyfriends lying concerns lies tell verbal expressions easy walking walk thats important end someone shows theyre capable believe hes capable manipulative deceitful even lies hes telling hell stop lying youre called denial see evidence something choose believe evidence might love heart love end verb based respectful loving behaviours wish start believing deserve\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "# Download NLTK stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv('train.csv')\n",
    "\n",
    "# Check for missing data\n",
    "print(\"Missing values in dataset:\\n\", data.isnull().sum())\n",
    "\n",
    "# Drop rows with missing values\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "\n",
    "\"\"\"nRowsRead = None # specify 'None' if want to read whole file\n",
    "data_train = pd.read_csv('train.csv', delimiter=',', nrows = nRowsRead)\n",
    "data_train.dataframeName = 'train.csv'\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data_train)\n",
    "# Check for missing values in 'Context' and 'Response' columns\n",
    "missing_context = df['Context'].isnull().sum()\n",
    "missing_response = df['Response'].isnull().sum()\n",
    "\n",
    "print(f'Missing values in Context: {missing_context}')\n",
    "print(f'Missing values in Response: {missing_response}')\n",
    "\n",
    "# Drop rows with missing 'Context' or 'Response'\n",
    "df.dropna(subset=['Context', 'Response'], inplace=True)\"\"\"\n",
    "\n",
    "# Preprocess text data\n",
    "def preprocess_text(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = ''.join([char for char in text if char not in string.punctuation])  # Remove punctuation\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])  # Remove stopwords\n",
    "    return text\n",
    "\n",
    "data['Context'] = data['Context'].apply(preprocess_text)\n",
    "data['Response'] = data['Response'].apply(preprocess_text)\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['Context'], data['Response'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a pipeline with TfidfVectorizer and LinearSVC\n",
    "model = make_pipeline(TfidfVectorizer(), LinearSVC())\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Function to generate responses\n",
    "def chatbot_response(user_input):\n",
    "    user_input = preprocess_text(user_input)\n",
    "    response = model.predict([user_input])\n",
    "    return response[0]\n",
    "\n",
    "# Test the chatbot\n",
    "user_input = \"Hello, how are you?\"\n",
    "print(\"Chatbot:\", chatbot_response(user_input))\n",
    "\n",
    "user_input = \"What is your name?\"\n",
    "print(\"Chatbot:\", chatbot_response(user_input))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bf35e9-685f-4f4d-8ca5-33aba928884d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in dataset:\n",
      " Context     0\n",
      "Response    4\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot is ready to chat! Type 'exit' to end the conversation.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  I'm going through some things with my feelings and myself. I barely sleep and I do nothing but think about how I'm worthless and how I shouldn't be here.    I've never tried or contemplated suicide. I've always wanted to fix my issues, but I never get around to it.    How can I change my feeling of being worthless to everyone?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: hi im sorry youre feeling way let see guide right direction often talk clients feelings worthlessness start little bit selfexploration start noticing start notice feelings come particular time day specific person brings phrase hear start notice usually tied something may take figure somethingsomeone try patientnext start explore feelings start think come something statement perhaps repeats head whose voice difficult questions take time answering usually sessions might even helpful write somewhere journal would great place research shown brain works differently put pen paper versus typing computer comes good news brains able rewire allows us change habits dont want well statements say longer serving us next step select ally someone corner someone always rooting dont someone like thats ok lot us dont make someone close eyes try describe person great detail way look way act way sound pick phrase would like person say whenever start think youre worthless something help feel better characteristic skill great joke tell physical attribute also takes time may involve asking help someone knows youonce together noticing answers feelings statements come ally new statement try put altogether feelings come notice bringing call upon ally try change statement head selfdefeating one positive uplifting one hope helpful clients quite weeks months whole time quite involved process bring lot difficult feelingmemories point find hard go alone please seek help take anything away reply know help possible change way feel\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "# Download NLTK stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv('train.csv')\n",
    "\n",
    "# Check for missing data\n",
    "print(\"Missing values in dataset:\\n\", data.isnull().sum())\n",
    "\n",
    "# Drop rows with missing values\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# Preprocess text data\n",
    "def preprocess_text(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = ''.join([char for char in text if char not in string.punctuation])  # Remove punctuation\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])  # Remove stopwords\n",
    "    return text\n",
    "\n",
    "data['Context'] = data['Context'].apply(preprocess_text)\n",
    "data['Response'] = data['Response'].apply(preprocess_text)\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['Context'], data['Response'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a pipeline with TfidfVectorizer and LinearSVC\n",
    "model = make_pipeline(TfidfVectorizer(), LinearSVC())\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Function to generate responses\n",
    "def chatbot_response(user_input):\n",
    "    user_input = preprocess_text(user_input)\n",
    "    response = model.predict([user_input])\n",
    "    return response[0]\n",
    "\n",
    "# Interactive loop to test the chatbot\n",
    "print(\"Chatbot is ready to chat! Type 'exit' to end the conversation.\")\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() == 'exit':\n",
    "        print(\"Chatbot: Goodbye!\")\n",
    "        break\n",
    "    response = chatbot_response(user_input)\n",
    "    print(\"Chatbot:\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fd0331-ea01-4d7c-b5be-f8f12035aece",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import re\n",
    "\n",
    "# Download NLTK stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv('train.csv')\n",
    "\n",
    "# Check for missing data\n",
    "print(\"Missing values in dataset:\\n\", data.isnull().sum())\n",
    "\n",
    "# Drop rows with missing values\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# Enhanced preprocessing function\n",
    "def preprocess_text(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])  # Remove stopwords\n",
    "    return text\n",
    "\n",
    "data['Context'] = data['Context'].apply(preprocess_text)\n",
    "data['Response'] = data['Response'].apply(preprocess_text)\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['Context'], data['Response'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a pipeline with TfidfVectorizer and LinearSVC\n",
    "model = make_pipeline(TfidfVectorizer(), LinearSVC())\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Function to generate responses\n",
    "def chatbot_response(user_input):\n",
    "    user_input = preprocess_text(user_input)\n",
    "    response = model.predict([user_input])\n",
    "    return response[0]\n",
    "\n",
    "# Interactive loop to test the chatbot\n",
    "print(\"Chatbot is ready to chat! Type 'exit' to end the conversation.\")\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() == 'exit':\n",
    "        print(\"Chatbot: Goodbye!\")\n",
    "        break\n",
    "    response = chatbot_response(user_input)\n",
    "    print(\"Chatbot:\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e7a34ab-c978-444e-bc3b-8a6b60e304be",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoModelForCausalLM, AutoTokenizer\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Load pre-trained model and tokenizer\u001b[39;00m\n\u001b[0;32m      5\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmicrosoft/DialoGPT-medium\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "model_name = \"microsoft/DialoGPT-medium\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Function to generate responses\n",
    "def chatbot_response(user_input):\n",
    "    # Tokenize the user input\n",
    "    new_user_input_ids = tokenizer.encode(user_input + tokenizer.eos_token, return_tensors='pt')\n",
    "    \n",
    "    # Get the chat history ids (append the new input)\n",
    "    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if 'chat_history_ids' in globals() else new_user_input_ids\n",
    "    \n",
    "    # Generate a response\n",
    "    chat_history_ids = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)\n",
    "    \n",
    "    # Decode the response\n",
    "    response = tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Interactive loop to test the chatbot\n",
    "print(\"Chatbot is ready to chat! Type 'exit' to end the conversation.\")\n",
    "chat_history_ids = None\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() == 'exit':\n",
    "        print(\"Chatbot: Goodbye!\")\n",
    "        break\n",
    "    response = chatbot_response(user_input)\n",
    "    print(\"Chatbot:\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe847c8-fa98-4797-abbb-aea0f594fe52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
